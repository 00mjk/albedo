{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T02:44:12.576357",
     "start_time": "2017-05-11T02:44:12.455685"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.param.shared import HasOutputCol\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T02:44:25.815272",
     "start_time": "2017-05-11T02:44:12.579253"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-----------------------+\n",
      "|count(id)|count(DISTINCT from_user_id)|count(DISTINCT repo_id)|\n",
      "+---------+----------------------------+-----------------------+\n",
      "|   441913|                        1100|                  93168|\n",
      "+---------+----------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'jdbc:sqlite:file:///Users/vinta/Projects/albedo/db.sqlite3'\n",
    "properties = {\n",
    "    'driver': 'org.sqlite.JDBC',\n",
    "    'date_string_format': 'yyyy-MM-dd HH:mm:ss'\n",
    "}\n",
    "\n",
    "rawDF = spark.read.jdbc(url, table='app_repostarring', properties=properties)\n",
    "rawDF.agg(F.count('id'), F.countDistinct('from_user_id'), F.countDistinct('repo_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-11T02:44:32.328939",
     "start_time": "2017-05-11T02:44:25.817585"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-----------------------+\n",
      "|count(id)|count(DISTINCT from_user_id)|count(DISTINCT repo_id)|\n",
      "+---------+----------------------------+-----------------------+\n",
      "|   691073|                        1583|                 198741|\n",
      "+---------+----------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'jdbc:mysql://127.0.0.1:3306/albedo'\n",
    "properties = {\n",
    "    'driver': 'com.mysql.jdbc.Driver',\n",
    "    'user': 'root',\n",
    "    'password': '123',\n",
    "}\n",
    "rawDF = spark.read.jdbc(url, table='app_repostarring', properties=properties)\n",
    "rawDF.agg(F.count('id'), F.countDistinct('from_user_id'), F.countDistinct('repo_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
